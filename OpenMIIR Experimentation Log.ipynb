{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##OpenMIIR Experimentation Log\n",
    "\n",
    "The goal of exploring the OpenMIIR github repository was to take a glance at how EEG data is generally pipelined for data cleaning.  In exploring the repository and testing a python notebook that pipelined sample data, the repository has a few dependencies that need to be addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies**\n",
    "\n",
    "The address of the GitHub repository https://github.com/sstober/openmiir, and the notebook I tested was in the directory \"/eeg/preprocessing/notebooks\".  I specifically tested the notebook \"Subject P01.ipynb\".  \n",
    "\n",
    "Once the github repository is cloned or downloaded and unzipped, make sure that the root folder is named \"OpenMIIR\", and if not, rename it; certain packages depend on the root folder having this name.\n",
    "\n",
    "First, download the raw .fif data.  There is a torrent file in the eeg folder.  Download the corresponding data file that the notebook processes through the torrent file.  For example, for the notebook I tested, the file is \"P01-raw.fif\".  Then, move the .fif file to the mne folder inside the eeg folder (the directory \"eeg/mne/\" from root).  The pipeline object will try to find the raw notebook file in the \"eeg/mne/\" directory.\n",
    "\n",
    "The preprocessing notebooks require the user to have two python modules to be installed: deepthought and pylearn2.  These can be found at https://github.com/sstober/deepthought and https://github.com/lisa-lab/pylearn2.  Once they are cloned or downloaded, add the root folder to a directory and add the directory path to the PYTHONPATH variable in the bash file (\".bash_profile\" in user directory for Macs) for the computer environment.\n",
    "\n",
    "The notebooks require at least two custom environment variables: DEEPTHOUGHT_DATA_PATH and DEEPTHOUGHT_OUTPUT_PATH.  To set these custom environment variables, add the following lines after importing os, replacing the parentheses with the directory that contains the OpenMIIR root folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DEEPTHOUGHT_DATA_PATH'] = \"(directory that contains the OpenMIIR root folder)\"\n",
    "os.environ['DEEPTHOUGHT_OUTPUT_PATH'] = \"(directory that contains the OpenMIIR root folder)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, when initializing the settings variable to pass to the Pipeline object, data_root must be set as the path to the OpenMIIR root folder from the notebook, which is \"../../../\".  Modify the line where the variable \"settings\" is set so that data_root=\"../../../\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "settings = dict(debug=False, mne_log_level='Info', sfreq=64, data_root=\"../../../\") # optional pipeline settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first notebook block should be modified to resemble something like the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "General workflow for importing a new session\n",
    "\"\"\"\n",
    "subject = 'P01' # TODO: change this for each subject\n",
    "verbose = True  # change this for debugging\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['DEEPTHOUGHT_DATA_PATH'] = \"/Users/mrincredible/Documents/pearson_lab\"\n",
    "os.environ['DEEPTHOUGHT_OUTPUT_PATH'] = \"/Users/mrincredible/Documents/pearson_lab\"\n",
    "print os.environ\n",
    "\n",
    "from deepthought.datasets.openmiir.preprocessing.pipeline import Pipeline\n",
    "settings = dict(debug=False, mne_log_level='Info', sfreq=64, data_root=\"../../../\") # optional pipeline settings\n",
    "pipeline = Pipeline(subject, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing notebooks and the deepthought/pylearn2 packages also require a number of other packages. However, they can all be installed through pip. Pip install the following packages:\n",
    "- Theano\n",
    "- watchdog\n",
    "- librosa\n",
    "\n",
    "After following the above steps, the notebook should run. If not, continue running the first notebook block and see what error python throws and what packages are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "In OpenMIIR, much of the pipelining code is actually contained in the MNE object.  The data is generally pipelined in two steps: bandpass filtering and ICA.  First, the time series EEG data is observed manually and checked for any bad channels.  Then, the pipeline interpolates the bad channels with its surrounding channels.\n",
    "\n",
    "Then, a power spectral density graph is plotted to check for unusual spikes at certain frequencies, and one channel is plotted to check for drift.  Then, a bandpass filter is applied to remove unnecessary frequencies (for this data, only frequencies in the 0.5-30Hz range were kept).\n",
    "\n",
    "This data also had EOG data, which checks for eye blinks and is used to check for artifacts by epoching the data at the eyeblinks.  After data cleaning, spikes at the epochs at eyeblinks should be reduced.\n",
    "\n",
    "ICA is performed to find bad components that are artifacts.  In order to perform ICA, the data is first downsampled.  Unfortunately, I tried to run the code for downsampling, but was never able to finish running the code.  However, if downsampling is not performed, performing ICA on the data also takes an extremely long time, so in terms of pipeline performance, downsampling may be a bottleneck if it is not optimized.\n",
    "\n",
    "After ICA is applied, the pipeline first finds components that correlate with the EOG data.  It then tries to find artifact components by checking for statistics associated with the component, such as skewness, kurtosis, and variance.\n",
    "\n",
    "The ICA data with the unwanted components excluded can then be saved through MNE, and also retrieved back and reconstructed into time series data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
